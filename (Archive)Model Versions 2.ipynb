{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64748d0-89bb-4ee4-8aac-921b230e214c",
   "metadata": {},
   "source": [
    "### With tokenization + lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac7b0c-dcd7-49c3-aaf7-cf7834807fcf",
   "metadata": {},
   "source": [
    "Note: Not apart of the modeling notebook, this is just an archive.\n",
    "\n",
    "The notebook is to test tokenization and lemmatization on my choosen models without EDA. I wanted to do this in a seperate notebook because I wanted to save my current work in the NLP, EDA and Models notebook as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26772c9-14ac-41f9-bb57-a66b0806a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimpy as skim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from wordcloud import WordCloud\n",
    "import unicodedata\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401f4a4b-211d-42d0-be21-caa0cd91aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing models with tokenization and lemmanization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e489481-83f7-4522-bc24-eb03dd0b3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main data\n",
    "skincare = pd.read_csv('data/skincare.csv')\n",
    "\n",
    "k_skin = pd.read_csv('data/koreanskincarereddit.csv')\n",
    "\n",
    "skin_add = pd.read_csv('data/skincareaddiction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d40533-4994-4096-897c-10b59b68dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = skincare['self_text']\n",
    "y = skincare['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "cvect_tokenized = CountVectorizer(tokenizer=tokenizer, max_features=1000)\n",
    "X_train_tokenized = cvect_tokenized.fit_transform(X_train)\n",
    "X_test_tokenized = cvect_tokenized.transform(X_test)\n",
    "feature_names_tokenized = cvect_tokenized.get_feature_names_out()\n",
    "\n",
    "def clean_feature_names(feature_names):\n",
    "    cleaned_names = [re.sub(r'\\W+', '', name) for name in feature_names]\n",
    "    cleaned_names = [re.sub(r'\\d+', '', name) for name in cleaned_names]\n",
    "    cleaned_names = [name for name in cleaned_names if len(name) > 1]\n",
    "    return cleaned_names\n",
    "\n",
    "cleaned_tokenized_names = list(set(clean_feature_names(feature_names_tokenized)))\n",
    "\n",
    "def is_latin(characters):\n",
    "    try:\n",
    "        name = unicodedata.name(characters)\n",
    "        if name.startswith('HANGUL') or name.startswith('HANGUL'):\n",
    "            return False\n",
    "        else:\n",
    "            return characters.isalpha()\n",
    "    except (TypeError, ValueError):\n",
    "        return False\n",
    "\n",
    "def filter_korean(text):\n",
    "    return ''.join(characters for characters in text if is_latin(characters))\n",
    "\n",
    "# Filter the training and test data\n",
    "X_train_filtered = [filter_korean(text) for text in X_train]\n",
    "X_test_filtered = [filter_korean(text) for text in X_test]\n",
    "\n",
    "# Define the stop words\n",
    "english_stop_words = list(CountVectorizer(stop_words='english').get_stop_words())\n",
    "custom_stop_words = ['ve', 'just', 'little', 'don']\n",
    "stop_words = english_stop_words + custom_stop_words\n",
    "\n",
    "# Create a new CountVectorizer with the cleaned feature names and stop words\n",
    "cvect_cleaned = CountVectorizer(vocabulary=cleaned_tokenized_names, stop_words=stop_words)\n",
    "X_train_cleaned = cvect_cleaned.fit_transform(X_train_filtered)\n",
    "X_test_cleaned = cvect_cleaned.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a9fe96-c727-428a-8756-bd0ea653873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'cvect__max_df': 0.75, 'cvect__min_df': 1, 'cvect__ngram_range': (1, 2), 'log_reg__C': 1}\n",
      "Best score found:\n",
      "0.7562913907284768\n"
     ]
    }
   ],
   "source": [
    "log_pipe = Pipeline([\n",
    "    ('cvect', CountVectorizer(max_features=10_000, stop_words=['stop_words'])),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('log_reg', LogisticRegression(max_iter=10_000))\n",
    "])\n",
    "\n",
    "log_pipe.fit(X_train, y_train)\n",
    "\n",
    "log_pipe.score(X_train, y_train)\n",
    "\n",
    "log_pipe.score(X_test, y_test)\n",
    "\n",
    "params = {\n",
    "    'cvect__max_df': (0.5, 0.75, 1.0),\n",
    "    'cvect__min_df': (1, 2, 3),\n",
    "    'cvect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'log_reg__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(log_pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gs.predict(X_test)\n",
    "y_pred \n",
    "\n",
    "gs.get_params()\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "gs.score(X_train, y_train)\n",
    "\n",
    "gs.score(X_test, y_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(gs.best_params_)\n",
    "print(\"Best score found:\")\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf40d8ee-ab92-4678-b1c6-28765a30e90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;cvect&#x27;,\n",
       "                 CountVectorizer(max_features=10000,\n",
       "                                 stop_words=[&#x27;stop_words&#x27;])),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;log_reg&#x27;, LogisticRegression(max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvect&#x27;,\n",
       "                 CountVectorizer(max_features=10000,\n",
       "                                 stop_words=[&#x27;stop_words&#x27;])),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;log_reg&#x27;, LogisticRegression(max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=10000, stop_words=[&#x27;stop_words&#x27;])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('cvect',\n",
       "                 CountVectorizer(max_features=10000,\n",
       "                                 stop_words=['stop_words'])),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('log_reg', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe = Pipeline([\n",
    "    ('cvect', CountVectorizer(max_features=10_000, stop_words=['stop_words'])),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('log_reg', LogisticRegression(max_iter=10_000))\n",
    "])\n",
    "\n",
    "log_pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e20ee7-5f51-41db-b761-abcbbc91b628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8980132450331125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d480723f-a37d-4683-b174-b40c7c50d644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7559523809523809"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "015ad224-c589-4920-8ab5-060a9aaf0c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'cvect__max_df': 0.75, 'cvect__min_df': 1, 'cvect__ngram_range': (1, 2), 'log_reg__C': 1, 'log_reg__penalty': 'l2'}\n",
      "Best score found:\n",
      "0.7562913907284768\n"
     ]
    }
   ],
   "source": [
    "X = skincare['self_text']\n",
    "y = skincare['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "#---------\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "cvect_tokenized = CountVectorizer(tokenizer=tokenizer, max_features=1000)\n",
    "X_train_tokenized = cvect_tokenized.fit_transform(X_train)\n",
    "X_test_tokenized = cvect_tokenized.transform(X_test)\n",
    "feature_names_tokenized = cvect_tokenized.get_feature_names_out()\n",
    "\n",
    "# # Lemmatization\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# def lemmatizer(sentence):\n",
    "#     doc = nlp(sentence)\n",
    "#     return [word.lemma_ for word in doc]\n",
    "\n",
    "# cvect_lemmatized = CountVectorizer(tokenizer=lemmatizer, max_features=1000)\n",
    "# X_train_lemmatized = cvect_lemmatized.fit_transform(X_train)\n",
    "# X_test_lemmatized = cvect_lemmatized.transform(X_test)\n",
    "# feature_names_lemmatized = cvect_lemmatized.get_feature_names_out()\n",
    "\n",
    "#----------\n",
    "\n",
    "def clean_feature_names(feature_names):\n",
    "\n",
    "    cleaned_names = [re.sub(r'\\W+', '', name) for name in feature_names]\n",
    "    \n",
    "\n",
    "    cleaned_names = [re.sub(r'\\d+', '', name) for name in cleaned_names]\n",
    "    \n",
    "\n",
    "    cleaned_names = [name for name in cleaned_names if len(name) > 1]\n",
    "    \n",
    "    return cleaned_names\n",
    "\n",
    "\n",
    "cleaned_tokenized_names = clean_feature_names(feature_names_tokenized)\n",
    "\n",
    "cleaned_lemmatized_names = clean_feature_names(feature_names_lemmatized)\n",
    "\n",
    "#--------------------\n",
    "def is_latin(characters):\n",
    "\n",
    "    try:\n",
    "        name = unicodedata.name(characters)\n",
    "        if name.startswith('HANGUL') or name.startswith('HANGUL'):\n",
    "            return False\n",
    "        else:\n",
    "            return characters.isalpha()\n",
    "    except (TypeError, ValueError):\n",
    "        return False\n",
    "\n",
    "def filter_korean(text):\n",
    "\n",
    "    return ''.join(characters for characters in text if is_latin(characters))\n",
    "\n",
    "#---------------------\n",
    "english_stop_words = list(CountVectorizer(stop_words='english').get_stop_words())\n",
    "custom_stop_words = ['ve', 'just', 'little', 'don']\n",
    "\n",
    "stop_words = english_stop_words + custom_stop_words\n",
    "\n",
    "cleaned_lemmatized_names = list(set(clean_feature_names(feature_names_lemmatized)))\n",
    "\n",
    "cvect_one = CountVectorizer(vocabulary=cleaned_lemmatized_names, stop_words=stop_words, lowercase=False)\n",
    "X_train_lemmatized = cvect_one.fit_transform(X_train)\n",
    "X_test_lemmatized = cvect_one.transform(X_test)\n",
    "#-------------------------------------\n",
    "log_pipe = Pipeline([\n",
    "    ('cvect', CountVectorizer(max_features=10_000, stop_words=['stop_words'])),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('log_reg', LogisticRegression(max_iter=10_000))\n",
    "])\n",
    "\n",
    "log_pipe.fit(X_train, y_train)\n",
    "\n",
    "log_pipe.score(X_train, y_train)\n",
    "\n",
    "log_pipe.score(X_test, y_test)\n",
    "\n",
    "params = {\n",
    "    'cvect__max_df': (0.5, 0.75, 1.0),\n",
    "    'cvect__min_df': (1, 2, 3),\n",
    "    'cvect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'log_reg__C': [0.1, 1, 10],\n",
    "    'log_reg__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(log_pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gs.predict(X_test)\n",
    "y_pred \n",
    "\n",
    "gs.get_params()\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "gs.score(X_train, y_train)\n",
    "\n",
    "gs.score(X_test, y_test)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(gs.best_params_)\n",
    "print(\"Best score found:\")\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5506338-b673-4969-a51d-4319ea166215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class_pipe = {\n",
    "    'K-Nearest Neighbors Classifier': KNeighborsClassifier(),\n",
    "    'Random Forest Classifier': RandomForestClassifier(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32a6647a-f750-493f-8a68-b10c90803d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for K-Nearest Neighbors Classifier:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Skincare_Addiction       0.55      0.69      0.61       241\n",
      "    koreanskincare       0.63      0.47      0.54       263\n",
      "\n",
      "          accuracy                           0.58       504\n",
      "         macro avg       0.59      0.58      0.57       504\n",
      "      weighted avg       0.59      0.58      0.57       504\n",
      "\n",
      "\n",
      "Results for Random Forest Classifier:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Skincare_Addiction       0.71      0.78      0.74       241\n",
      "    koreanskincare       0.78      0.71      0.74       263\n",
      "\n",
      "          accuracy                           0.74       504\n",
      "         macro avg       0.74      0.74      0.74       504\n",
      "      weighted avg       0.75      0.74      0.74       504\n",
      "\n",
      "\n",
      "Results for Naive Bayes:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Skincare_Addiction       0.78      0.72      0.75       241\n",
      "    koreanskincare       0.76      0.81      0.78       263\n",
      "\n",
      "          accuracy                           0.77       504\n",
      "         macro avg       0.77      0.77      0.77       504\n",
      "      weighted avg       0.77      0.77      0.77       504\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_model_class_pipe = {}\n",
    "for name, classifier in model_class_pipe.items():\n",
    "    pipe = Pipeline([\n",
    "        ('countvect', CountVectorizer()),\n",
    "        ('model', classifier)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    results_model_class_pipe[name] = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "for name, report in results_model_class_pipe.items():\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(report)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
